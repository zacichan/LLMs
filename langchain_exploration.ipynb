{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmwj1Qg7reEG+A7pLXOEQw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zacichan/LLMs/blob/dev/langchain_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to the OpenAI API\n"
      ],
      "metadata": {
        "id": "Zw0XMbPsuQcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having a conversation with GPT involves a single function call of this form.\n",
        "\n",
        "```python\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"MODEL_NAME\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": 'SPECIFY HOW THE AI ASSISTANT SHOULD BEHAVE'},\n",
        "        {\"role\": \"user\", \"content\": 'SPECIFY WANT YOU WANT THE AI ASSISTANT TO SAY'}\n",
        "    ]\n",
        ")\n",
        "```\n",
        "\n",
        "There are a few things to unpack here.\n",
        "\n",
        "The model names are listed in the [Model Overview](https://platform.openai.com/docs/models/overview) page of the developer documentation. We will use gpt-3.5-turbo, which is (as of March 2023) the latest model used by ChatGPT that has broad public API access.\n",
        "\n",
        "There are three types of message, documented in the Introduction to the Chat documentation: \n",
        "\n",
        "*   system messages describe the behavior of the AI assistant. \n",
        "*   user messages describe what you want the AI assistant to say.\n",
        "*   assistant messages describe previous responses in the conversation.\n",
        "\n",
        "The first message should be a system message. Additional messages should alternate between user and assistant."
      ],
      "metadata": {
        "id": "QQEviElEyhw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "!pip install openai\n",
        "!pip install yfinance\n",
        "!pip install IPython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfHEyvXRDjJE",
        "outputId": "496f823b-e5b6-4d2c-951c-fe6b83a6e1b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.4 yarl-1.8.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.9/dist-packages (0.2.14)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.9/dist-packages (from yfinance) (2022.7.1)\n",
            "Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.9/dist-packages (from yfinance) (2.27.1)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.9/dist-packages (from yfinance) (4.9.2)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: cryptography>=3.3.2 in /usr/local/lib/python3.9/dist-packages (from yfinance) (40.0.1)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.22.4)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.9/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.9/dist-packages (from yfinance) (4.11.2)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.9/dist-packages (from yfinance) (2.3.6)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography>=3.3.2->yfinance) (1.15.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.9/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (2022.12.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance) (2.21)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.9/dist-packages (7.34.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from IPython) (4.8.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from IPython) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from IPython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from IPython) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from IPython) (3.0.38)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.9/dist-packages (from IPython) (5.7.1)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.9/dist-packages (from IPython) (0.1.6)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from IPython) (2.14.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from IPython) (67.6.1)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->IPython) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect>4.3->IPython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython) (0.2.6)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.18.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the os package\n",
        "import os\n",
        "\n",
        "# Import the openai package\n",
        "import openai\n",
        "\n",
        "# Import yfinance as yf\n",
        "import yfinance as yf\n",
        "\n",
        "# From the IPython.display package, import display and Markdown\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Set openai.api_key to the OPENAI environment variable\n",
        "openai.api_key = \"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\""
      ],
      "metadata": {
        "id": "vVUpxV_OAZz5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using GPT3 as a helpful assistant\n",
        "\n",
        "We define our system message as:\n",
        "\n",
        "*You are a helpful assistant who understands data science.*\n",
        "\n",
        "And our first user message as:\n",
        "\n",
        "*Create a small dataset of data about people. The format of the dataset should be a data frame with 5 rows and 3 columns. The columns should be called \"name\", \"height_cm\", and \"eye_color\". The \"name\" column should contain randomly chosen first names. The \"height_cm\" column should contain randomly chosen heights, given in centimeters. The \"eye_color\" column should contain randomly chosen eye colors, taken from a choice of \"brown\", \"blue\", and \"green\". Provide Python code to generate the dataset, then provide the output in the format of a markdown table*"
      ],
      "metadata": {
        "id": "2WKDfEOS1NVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the system message\n",
        "system_msg = 'You are a helpful assistant who understands data science.'\n",
        "\n",
        "# Define the user message\n",
        "user_msg = 'Create a small dataset of data about people. The format of the dataset should be a data frame with 5 rows and 3 columns. The columns should be called \"name\", \"height_cm\", and \"eye_color\". The \"name\" column should contain randomly chosen first names. The \"height_cm\" column should contain randomly chosen heights, given in centimeters. The \"eye_color\" column should contain randomly chosen eye colors, taken from a choice of \"brown\", \"blue\", and \"green\". Provide Python code to generate the dataset, then provide the output in the format of a markdown table.'\n",
        "\n",
        "# Create a dataset using GPT\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_msg},\n",
        "        {\"role\": \"user\", \"content\": user_msg}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "ajy6IuBhEYy0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "API calls are considered risky as they can lead to problems that may not be confined to the notebook such as connectivity issues, server errors or inadequate API credits. It is advisable to check the response received from the server to ensure its accuracy.\n",
        "\n",
        "When using GPT models, a status code is returned in the response after API calls. These status codes include 'stop' which indicates that the API has returned complete model output, 'length' which is indicative of incomplete model output due to parameter limits, 'content_filter' which denotes content that has been omitted due to content filter flags, and 'null' which is indicative of an API response still in progress or incomplete. These codes are documented in the Response format section of the Chat documentation.\n",
        "\n",
        "The response from the GPT API is sent in JSON format, resulting in deeply nested lists and dictionaries that can be challenging to work with.\n",
        "\n"
      ],
      "metadata": {
        "id": "yGoIir8z3Cwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Use `pd.json_normalize()` to transform the `response` object into a Pandas DataFrame\n",
        "# The DataFrame will contain one row for each choice made by the chatbot\n",
        "# The columns will include the choice ID, object, creation time, model, and usage information\n",
        "choices_df = pd.json_normalize(\n",
        "    response,  # The response object returned by the OpenAI API\n",
        "    \"choices\",  # The path to the nested list of choices in the response object\n",
        "    ['id', 'object', 'created', 'model', 'usage']  # A list of top-level keys to include in the DataFrame\n",
        ")\n",
        "\n",
        "choices_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "PQFBHefFJh0C",
        "outputId": "41427c1f-6fc8-4019-91fe-818043dd6674"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  finish_reason  index message.role  \\\n",
              "0          stop      0    assistant   \n",
              "\n",
              "                                     message.content  \\\n",
              "0  Sure, here's the code to generate the dataset:...   \n",
              "\n",
              "                                       id           object     created  \\\n",
              "0  chatcmpl-72My9J2HBhZj0lXX1QLsV2CNY6eUh  chat.completion  1680799305   \n",
              "\n",
              "                model                                              usage  \n",
              "0  gpt-3.5-turbo-0301  {'prompt_tokens': 145, 'completion_tokens': 22...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f7706d3e-0b1c-4cfe-91b5-f6c093db369f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>finish_reason</th>\n",
              "      <th>index</th>\n",
              "      <th>message.role</th>\n",
              "      <th>message.content</th>\n",
              "      <th>id</th>\n",
              "      <th>object</th>\n",
              "      <th>created</th>\n",
              "      <th>model</th>\n",
              "      <th>usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>stop</td>\n",
              "      <td>0</td>\n",
              "      <td>assistant</td>\n",
              "      <td>Sure, here's the code to generate the dataset:...</td>\n",
              "      <td>chatcmpl-72My9J2HBhZj0lXX1QLsV2CNY6eUh</td>\n",
              "      <td>chat.completion</td>\n",
              "      <td>1680799305</td>\n",
              "      <td>gpt-3.5-turbo-0301</td>\n",
              "      <td>{'prompt_tokens': 145, 'completion_tokens': 22...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7706d3e-0b1c-4cfe-91b5-f6c093db369f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f7706d3e-0b1c-4cfe-91b5-f6c093db369f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f7706d3e-0b1c-4cfe-91b5-f6c093db369f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To retrieve the status code from the response variable, you can access it using the expression `response[\"choices\"][0][\"finish_reason\"]`."
      ],
      "metadata": {
        "id": "KJf7uEzQ2yaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the status code of the response variable\n",
        "response[\"choices\"][0][\"finish_reason\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QJxICKc6JvJa",
        "outputId": "550b2278-3023-4858-eee4-53a025b16f87"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'stop'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the content generated by GPT.\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCW-zsTWJx5v",
        "outputId": "c113d93b-8694-4921-b159-9c2a6c4b3ecb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here's the code to generate the dataset:\n",
            "```python\n",
            "import random\n",
            "import pandas as pd\n",
            "\n",
            "names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Emily\"]\n",
            "heights = [165, 170, 175, 180, 185]\n",
            "eye_colors = [\"brown\", \"blue\", \"green\"]\n",
            "\n",
            "data = {\"name\": [random.choice(names) for _ in range(5)],\n",
            "        \"height_cm\": [random.choice(heights) for _ in range(5)],\n",
            "        \"eye_color\": [random.choice(eye_colors) for _ in range(5)]}\n",
            "\n",
            "df = pd.DataFrame(data)\n",
            "\n",
            "print(df.to_markdown(index=False))\n",
            "```\n",
            "\n",
            "And here's the output in markdown table format:\n",
            "| name     |   height_cm | eye_color   |\n",
            "|:--------|------------:|:-----------|\n",
            "| Alice   |         170 | brown      |\n",
            "| Emily   |         175 | green      |\n",
            "| Emily   |         180 | brown      |\n",
            "| Charlie |         165 | brown      |\n",
            "| Bob     |         170 | blue       |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also define a helper function to streamline our process"
      ],
      "metadata": {
        "id": "P8fy8hCxbObJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function to avoid boiler plate code.\n",
        "\n",
        "def chat(system, user_assistant):\n",
        "    # Check that the inputs are of the expected types\n",
        "    assert isinstance(system, str), \"`system` should be a string\"\n",
        "    assert isinstance(user_assistant, list), \"`user_assistant` should be a list\"\n",
        "    \n",
        "    # Create a dictionary representing the initial message from the system\n",
        "    system_msg = {\"role\": \"system\", \"content\": system}\n",
        "    \n",
        "    # Create a list of dictionaries representing the conversation between the user and assistant\n",
        "    user_assistant_msgs = [\n",
        "        {\"role\": \"assistant\", \"content\": user_assistant[i]} if i % 2 else {\"role\": \"user\", \"content\": user_assistant[i]} \n",
        "        for i in range(len(user_assistant))\n",
        "    ]\n",
        "    \n",
        "    # Combine the system message and the conversation messages into a single list\n",
        "    msgs = [system_msg] + user_assistant_msgs\n",
        "    \n",
        "    # Call the OpenAI API to get a response from the chatbot\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",  # Specify the version of the GPT-3.5 model to use\n",
        "        messages=msgs  # Pass the list of messages to the API as input\n",
        "    )\n",
        "    \n",
        "    # Check that the chatbot finished generating a response\n",
        "    status_code = response[\"choices\"][0][\"finish_reason\"]\n",
        "    assert status_code == \"stop\", f\"The status code was {status_code}.\"\n",
        "    \n",
        "    # Return the generated response\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n"
      ],
      "metadata": {
        "id": "ObiWiwEUL8wv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_fn_test = chat(\n",
        "    \"You are a machine learning expert who writes tersely.\", \n",
        "    [\"Explain what Git is, but use the analogy of a family tree\"]\n",
        ")\n",
        "display(Markdown(response_fn_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "vPb3gsTtMRLz",
        "outputId": "03d63e6b-dd73-413c-aaaf-9fe59561f97d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Git is like a big fish that saves snapshots of the pond at different points in time. Just like how a big fish can live for a long time and witness changes in the pond's ecosystem, Git allows you to track changes in your codebase. Each snapshot, or \"commit\", captures the current state of the code at a specific point in time. These commits can be compared to see how the codebase has changed over time, similar to how you can compare the snapshots of the pond to see how its ecosystem has evolved. Git also allows multiple people to work on the same codebase without overwriting each other's changes, just like how different fish can coexist and swim in the same pond."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the content from the response in Task 1 to assistant_msg\n",
        "assistant_msg = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "# Define a new user message\n",
        "user_msg2 = 'Using the dataset you just created, write code to calculate the mean of the `height_cm` column. Also include the result of the calculation.'\n",
        "\n",
        "# Create an array of user and assistant messages\n",
        "user_assistant_msgs = [user_msg, assistant_msg, user_msg2]\n",
        "\n",
        "# Get GPT to perform the request\n",
        "response_calc = chat(system_msg, user_assistant_msgs)\n",
        "\n",
        "# Display the generated content\n",
        "display(Markdown(response_calc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "32_spEl9N0jx",
        "outputId": "ff339a88-203f-4d4d-c738-62729d10189d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sure, here's the code to calculate the mean of the `height_cm` column:\n```python\nprint(\"Mean height: \", df[\"height_cm\"].mean())\n```\n\nAnd here's the output with the mean height:\n```\nMean height:  172.0\n``` \n\nSo the mean height is 172cm."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Silicon Valley Bank stock data from Yahoo! Finance:"
      ],
      "metadata": {
        "id": "W8NXYsOfvo8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Ticker object for SIVB\n",
        "sivb = yf.Ticker(\"SIVB\")\n",
        "\n",
        "# Get the stock history for SIVB for the period of 1 month\n",
        "sivb_history = sivb.history(period=\"1mo\")\n",
        "\n",
        "# Select the Close column and round it to two decimal places\n",
        "sivb_close = sivb_history[[\"Close\"]].round(2)"
      ],
      "metadata": {
        "id": "6gD3NQ_owASq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get GPT to write a financial report"
      ],
      "metadata": {
        "id": "PnHJC-WCwHYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a system message\n",
        "system_msg_sivb = 'You are a financial data expert who writes tersely.'\n",
        "\n",
        "# Define a user message (including the dataset)\n",
        "user_msg_sivb = '''The closing prices for the Silicon Valley Bank stock (ticker SIVB) are provided below. Provide Python code to analyze the data including the following metrics:\n",
        "\n",
        "- The date of the highest closing price.\n",
        "- The date of the lowest closing price.\n",
        "- The date with the largest change from the previous closing price.\n",
        "\n",
        "Also write a short report that includes the results of the calculations.\n",
        "\n",
        "Here is the dataset:\n",
        "\n",
        "''' + sivb_close.to_string()\n",
        "\n",
        "# Get GPT to generate a response\n",
        "response_sivb = chat(system_msg_sivb, [user_msg_sivb])\n",
        "\n",
        "# Render the response as Markdown\n",
        "display(Markdown(response_sivb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "PMp7pbNdwLum",
        "outputId": "257ee935-df8c-41e1-d9c2-0fc78667a34e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\nimport pandas as pd\n\ndata = pd.read_csv('sivb_data.csv')\ndata['Date'] = pd.to_datetime(data['Date'])\n\n# Highest closing price date\nhigh_close = data[data['Close'] == data['Close'].max()]['Date']\nprint('Date of highest closing price:', high_close.values[0])\n\n# Lowest closing price date\nlow_close = data[data['Close'] == data['Close'].min()]['Date']\nprint('Date of lowest closing price:', low_close.values[0])\n\n# Date with largest change from previous closing price\ndata['Change'] = data['Close'].diff()\nmax_change = data[data['Change'] == data['Change'].max()]['Date']\nprint('Date of largest change from previous closing price:', max_change.values[0])\n```\n\nReport:\n\n- The date of the highest closing price is March 6th, 2023.\n- The date of the lowest closing price is March 28th, 2023.\n- The date with the largest change from the previous closing price is March 29th, 2023. On this day, the closing price increased by $0.57 from the previous day's closing price of $0.40. This represents a percent increase of 142.5%, which is an unusually large increase. It is possible that there was some sort of corporate announcement or other event that caused the large increase."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "DlEc5GxCx8Wf",
        "outputId": "13529948-ac26-400a-f822-a41036a7d2f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"```python\\nimport pandas as pd\\n\\ndata = pd.read_csv('sivb_data.csv')\\ndata['Date'] = pd.to_datetime(data['Date'])\\n\\n# Highest closing price date\\nhigh_close = data[data['Close'] == data['Close'].max()]['Date']\\nprint('Date of highest closing price:', high_close.values[0])\\n\\n# Lowest closing price date\\nlow_close = data[data['Close'] == data['Close'].min()]['Date']\\nprint('Date of lowest closing price:', low_close.values[0])\\n\\n# Date with largest change from previous closing price\\ndata['Change'] = data['Close'].diff()\\nmax_change = data[data['Change'] == data['Change'].max()]['Date']\\nprint('Date of largest change from previous closing price:', max_change.values[0])\\n```\\n\\nReport:\\n\\n- The date of the highest closing price is March 6th, 2023.\\n- The date of the lowest closing price is March 28th, 2023.\\n- The date with the largest change from the previous closing price is March 29th, 2023. On this day, the closing price increased by $0.57 from the previous day's closing price of $0.40. This represents a percent increase of 142.5%, which is an unusually large increase. It is possible that there was some sort of corporate announcement or other event that caused the large increase.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}